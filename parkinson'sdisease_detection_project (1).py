# -*- coding: utf-8 -*-
"""Parkinson'sDisease Detection Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mpil9NhSC2R0ZNf-686kx324xnMPppTB
"""



"""Importing Dependencies"""

import numpy as np 
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn import svm 
from sklearn.metrics import accuracy_score

"""Data collection and preprocessing"""

# loading the data from csv file to a pandas dataframe
parkinson_data= pd.read_csv("/content/parkinsons.csv.data")

# printing the first 5 rows of the dataframe
parkinson_data.head()

# number of rows and columns in a dataframe
parkinson_data.shape

# getting more information about dataset
parkinson_data.info()

#checking of missing value in each column
parkinson_data.isnull().sum()

# getting some statistical measures of data 
parkinson_data.describe()

# distribution of target variable
parkinson_data['status'].value_counts()

"""1 ---> parkinsons positive


0 ---> Healthy
"""

# grouping the data based on target variable
parkinson_data.groupby('status').mean()

"""Data preprocessing"""

# seperating the feature & target
x = parkinson_data.drop(columns = ['name','status'],axis = 1)
y = parkinson_data['status']

print(x)
print(y)

"""spliting the data into training data & test data"""

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state = 2)

print(x.shape,x_train.shape,x_test.shape)



"""Data standardization"""

scaler = StandardScaler()

scaler.fit(x_train)

x_train = scaler.transform(x_train)

x_test = scaler.transform(x_test)

print(x_train)

""" Model Training

 support vector machine
"""

model = svm.SVC(kernel = 'linear')

# training the svm model with training data
model.fit(x_train,y_train)

"""Model Evaluation 

Accuracy score
"""

#accuracy score on the training data
x_train_prediction = model.predict(x_train)
training_data_accuracy = accuracy_score(x_train_prediction,y_train)

print("Accuracy score of training data : ", training_data_accuracy)

#accuracy score on the testing data
x_test_prediction = model.predict(x_test)
testing_data_accuracy = accuracy_score(x_test_prediction,y_test)

print("Accuracy score of test data : ", testing_data_accuracy)

"""Building a predictive system"""

input_data = (152.84500,163.30500,75.83600,0.00294,0.00002,0.00121,0.00149,0.00364,0.01828,0.15800,0.01064,0.00972,0.01246,0.03191,0.00609,24.92200,0.474791,0.654027,-6.105098,0.203653,2.125618,0.170100)

# changing input data as numpy array
input_data_as_numpy_array = np.asarray(input_data)

# reshape the numpy array for using one instance
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)

# standardize the data
std_data = scaler.transform(input_data_reshaped)

prediction = model.predict(std_data)

print(prediction)
if(prediction[0]==0):
  print("The person does not have parkinson's Disease")
else:
   print("The person has parkinson's Disease")

